output_dir: "output/image_lora"
pretrained_model_path: "./stable-diffusion-v1-5"
unet_subfolder: "unet_webvidlora_v3"

train_data:
  num_frames: 8
  sample_stride: 6
  is_image: true

validation_data:
  prompts:
    - "take plate in the kitchen with hands, egocentric view, first person"
    - "put down plate in the kitchen with hands, egocentric view, first person"
    - "take paper in the kitchen with hands, egocentric view, first person"
    - "squeeze cloth in the kitchen with hands, egocentric view, first person"
    - "close bin in the kitchen with hands, egocentric view, first person"
    - "close fridge in the kitchen with hands, egocentric view, first person"
    - "cut pizza in the kitchen with hands, egocentric view, first person"
    - "move chair in the kitchen with hands, egocentric view, first person"
  num_inference_steps: 50
  guidance_scale: 8.

noise_scheduler_kwargs:
  num_train_timesteps: 1000
  beta_start: 0.00085
  beta_end: 0.012
  beta_schedule: "scaled_linear"
  steps_offset: 1
  clip_sample: false

do_sanity_check:      true
max_train_epoch:      -1
max_train_steps:      10000
validation_steps:       2000
validation_steps_tuple: [2,]

learning_rate:    1.e-4

lora_rank: 2

num_workers: 8
train_batch_size: 16
gradient_accumulation_steps: 16

checkpointing_epochs: -1
checkpointing_steps:  2000

mixed_precision_training: true
enable_xformers_memory_efficient_attention: false

global_seed: 42
logger_interval: 10
